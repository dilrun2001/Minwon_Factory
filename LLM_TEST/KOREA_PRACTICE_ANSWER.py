from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

# âœ… base ëª¨ë¸ ë¡œë“œ (Polyglot-ko)
base_model = "EleutherAI/polyglot-ko-1.3b"
tokenizer = AutoTokenizer.from_pretrained(base_model)
base = AutoModelForCausalLM.from_pretrained(base_model, device_map="auto", torch_dtype=torch.float16)

# âœ… LoRA ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (í•™ìŠµí•œ ìµœì‹  ê²½ë¡œë¡œ ë°”ê¿”ì¤˜!)
adapter_path = "./polyglot-ko-minwon-model/checkpoint-870"

# âœ… í•™ìŠµëœ LoRA ì–´ëŒ‘í„° ì ìš©
model = PeftModel.from_pretrained(base, adapter_path).to("cuda")
model.eval()

# âœ… ë¯¼ì› ìƒì„± í•¨ìˆ˜ ì •ì˜ (RAM ì•ˆì •í™” ì½”ë“œ í¬í•¨!)
def generate_minwon_reply(instruction, max_new_tokens=200):
    prompt = f"ë¯¼ì› ë‚´ìš©:\n{instruction.strip()}\n\në‹µë³€:"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    with torch.no_grad():  # ğŸ’¡ í•™ìŠµ ì•„ë‹˜, ì¶”ë¡ ì´ë¯€ë¡œ no_grad ì‚¬ìš©!
        outputs = model.generate(
            inputs["input_ids"],
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )

    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    final_reply = output_text.split("ë‹µë³€:")[-1].strip()

    # âœ… ë©”ëª¨ë¦¬ ì •ë¦¬!
    del inputs, outputs
    torch.cuda.empty_cache()

    print("ğŸ“ ì…ë ¥ í”„ë¡¬í”„íŠ¸:\n", prompt)
    print("\nğŸ“¤ ì „ì²´ ìƒì„± ê²°ê³¼:\n", output_text)
    print("\nğŸ¤– ìµœì¢… ì¶”ì¶œëœ ë‹µë³€:\n", final_reply)

    return final_reply


# âœ… ì˜ˆì‹œ ë¯¼ì› í…ŒìŠ¤íŠ¸
test_input = """
í•˜ë‹¨ì‹œì¥ ê³µì˜ì£¼ì°¨ì¥ì— ë¶ˆë²•ì£¼ì°¨ ì°¨ëŸ‰ì´ ë§ì•„ ì§„ì…ì´ ì–´ë µìŠµë‹ˆë‹¤.
ë‹¨ì† ë° ì•ˆë‚´ ê°•í™” ìš”ì²­ë“œë¦½ë‹ˆë‹¤.
"""

# ğŸš€ ì¶”ë¡  ì‹¤í–‰ (ì—¬ëŸ¬ ë²ˆ í•´ë„ ì•ˆì •!)
generate_minwon_reply(test_input)