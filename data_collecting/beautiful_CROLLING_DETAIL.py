import re
import json
import requests
from bs4 import BeautifulSoup


##bs 4 ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì§ˆì˜ ì‘ë‹µ ë¶€ë¶„ì„ ê°€ì ¸ì˜¤ê²Œ ì²˜ë¦¬ í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤ .


# ğŸ”— ëª©ë¡ ì¡°íšŒ URL
list_url = "https://eminwon.saha.go.kr/emwp/gov/mogaha/ntis/web/emwp/cns/action/EmwpCnslWebAction.do"

# ğŸ“Œ ìš”ì²­ í—¤ë” ì„¤ì •
headers = {
    "User-Agent": "Mozilla/5.0",
    "Content-Type": "application/x-www-form-urlencoded",
}

# âœ… ì „ì²´ ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
tinyllama_data = []

# âœ… í˜ì´ì§€ ë°˜ë³µ ì–¼ë§ˆë‚˜ ì§„í–‰í• ì§€ ì‘ì„±

#ë°ì´í„° ë””ë²„ê¹… ìš©
for page in range(1, 276): # 2025 ê¹Œì§€ì˜ ë°ì´í„° í•©ìŠµ í•´ì•¼ í•œë‹¤.
    print(f"ğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")

    list_data = {
        "method": "selectCnslWebPage",
        "menu_id": "EMWPCnslWebInqL",
        "jndinm": "EmwpCnslWebEJB",
        "methodnm": "selectCnslWebPage",
        "context": "NTIS",
        "pageIndex": str(page),
        "pageSize": "20"
    }

    # ì˜¤ë¥˜ ë°œìƒì‹œ ì²˜ë¦¬ ë°©ë²•
    response = requests.post(list_url, data=list_data, headers=headers)
    if response.status_code != 200:
        print(f"âŒ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨!")
        break

    soup = BeautifulSoup(response.text, "html.parser")
    rows = soup.select("table.table tbody tr")

    if not rows:
        print("â›” ë” ì´ìƒ ë°ì´í„° ì—†ìŒ! ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        break

    for row in rows:
        cols = row.find_all("td")
        if len(cols) >= 2:
            title = cols[1].text.strip()
            onclick = cols[1].find("a")["href"]
            match = re.search(r"fncViewDtl\('(\d+)'", onclick)
            if match:
                cnsl_qna_no = match.group(1)

                detail_data = {
                    "method": "selectCnslWebShow",
                    "jndinm": "EmwpCnslWebEJB",
                    "methodnm": "selectCnslWebShow",
                    "context": "NTIS",
                    "cnsl_qna_no": cnsl_qna_no,
                }

                detail_resp = requests.post(list_url, data=detail_data, headers=headers)
                if detail_resp.status_code == 200:
                    detail_soup = BeautifulSoup(detail_resp.text, "html.parser")
                    tables = detail_soup.select("table.bbs-table-view")

                    question_text = tables[0].find_all("tr")[-1].get_text(separator="\n", strip=True) if len(tables) > 0 else ""
                    answer_text = tables[1].find_all("tr")[-1].get_text(separator="\n", strip=True) if len(tables) > 1 else ""

                    #  ì§ˆë¬¸/ë‹µë³€ë§Œ ì‚¬ìš©, ê³µë°± ë˜ëŠ” í˜•ì‹ ì˜¤ë¥˜ ë°ì´í„° ì œê±°
                    if question_text.strip() and answer_text.strip():
                        data_item = {
                            "instruction": question_text.strip(),  # ë¯¼ì› ë‚´ìš©ë§Œ!
                            "output": answer_text.strip()          # ì‹¤ì œ ë‹µë³€ë§Œ!
                        }
                        tinyllama_data.append(data_item)
                        print("âœ… ë¯¼ì› ìˆ˜ì§‘:", title)

# âœ… JSONL íŒŒì¼ë¡œ ì €ì¥ (ê° ì¤„ë§ˆë‹¤ í•˜ë‚˜ì˜ JSON ê°ì²´)
with open("data.jsonl", "w", encoding="utf-8") as f:
    for item in tinyllama_data:
        json_line = json.dumps(item, ensure_ascii=False)
        f.write(json_line + "\n")

print(f"ğŸ‰ ì´ {len(tinyllama_data)}ê°œì˜ ë°ì´í„°ê°€ tinyllama_data.jsonlì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")